"use strict";exports.id=9567,exports.ids=[9567],exports.modules={3506:(e,t,s)=>{s.d(t,{S:()=>o});class a{constructor(e){this.instances=new Map,this.currentIndex=0,this.HEALTH_CHECK_INTERVAL=3e4,this.HEALTH_CHECK_TIMEOUT=5e3,this.MAX_CONSECUTIVE_FAILURES=3,this.LOAD_DECAY_FACTOR=.9,this.RESPONSE_TIME_WEIGHT=.4,this.LOAD_SCORE_WEIGHT=.3,this.SUCCESS_RATE_WEIGHT=.3,this.stats={totalRequests:0,totalErrors:0,averageResponseTime:0,healthyInstances:0,totalInstances:0,lastRebalance:Date.now()},e.forEach(e=>{this.instances.set(e,{url:e,healthy:!0,lastCheck:0,responseTime:1e3,loadScore:0,errorCount:0,successCount:0,consecutiveFailures:0,models:[]})}),setInterval(()=>this.performHealthChecks(),this.HEALTH_CHECK_INTERVAL),setInterval(()=>this.decayLoadScores(),6e4),console.log(`üîÑ Advanced Load Balancer initialized with ${e.length} instances`)}async getBestInstance(){this.stats.totalRequests++;let e=Array.from(this.instances.entries()).filter(([e,t])=>t.healthy).sort(([e,t],[s,a])=>this.calculateInstanceScore(t)-this.calculateInstanceScore(a));if(0===e.length)return console.error("‚ö†Ô∏è No healthy instances available, falling back to primary"),Array.from(this.instances.keys())[0];let[t,s]=e[0];return s.loadScore+=1,this.updateStats(),console.log(`üéØ Selected instance: ${t} (score: ${this.calculateInstanceScore(s).toFixed(3)})`),t}async getBestInstanceForModel(e){this.stats.totalRequests++;let t=Array.from(this.instances.entries()).filter(([t,s])=>s.healthy&&s.models.includes(e)).sort(([e,t],[s,a])=>this.calculateInstanceScore(t)-this.calculateInstanceScore(a));if(0===t.length){console.error(`‚ö†Ô∏è No healthy instances available with model '${e}', falling back to any healthy instance`);let t=Array.from(this.instances.entries()).filter(([e,t])=>t.healthy).sort(([e,t],[s,a])=>this.calculateInstanceScore(t)-this.calculateInstanceScore(a));if(t.length>0){let[e,s]=t[0];return s.loadScore+=1,this.updateStats(),e}return Array.from(this.instances.keys())[0]}let[s,a]=t[0];return a.loadScore+=1,this.updateStats(),console.log(`üéØ Selected instance for model '${e}': ${s} (score: ${this.calculateInstanceScore(a).toFixed(3)})`),s}calculateInstanceScore(e){let t=e.responseTime/1e3,s=e.loadScore,a=e.successCount/Math.max(1,e.successCount+e.errorCount);return t*this.RESPONSE_TIME_WEIGHT+s*this.LOAD_SCORE_WEIGHT+(1-a)*this.SUCCESS_RATE_WEIGHT}recordRequestResult(e,t,s){let a=this.instances.get(e);a&&(a.responseTime=.7*a.responseTime+.3*t,s?(a.successCount++,a.consecutiveFailures=0,a.healthy||0!==a.consecutiveFailures||(console.log(`‚úÖ Instance ${e} recovered, marking as healthy`),a.healthy=!0)):(a.errorCount++,a.consecutiveFailures++,this.stats.totalErrors++,a.consecutiveFailures>=this.MAX_CONSECUTIVE_FAILURES&&(console.log(`‚ùå Instance ${e} marked unhealthy after ${a.consecutiveFailures} failures`),a.healthy=!1)))}async performHealthChecks(){let e=Array.from(this.instances.entries()).map(async([e,t])=>{let s=Date.now();try{let a=new AbortController,o=setTimeout(()=>a.abort(),this.HEALTH_CHECK_TIMEOUT),n=await fetch(`${e}/api/tags`,{method:"GET",signal:a.signal});clearTimeout(o);let r=Date.now()-s;if(n.ok){this.recordRequestResult(e,r,!0);try{let e=await n.json();Array.isArray(e.models)?t.models=e.models.map(e=>e.name):t.models=[]}catch(e){t.models=[]}t.healthy||console.log(`üîÑ Instance ${e} is responding again`)}else this.recordRequestResult(e,r,!1),t.models=[];t.lastCheck=Date.now()}catch(o){let a=Date.now()-s;this.recordRequestResult(e,a,!1),t.lastCheck=Date.now(),t.models=[],console.log(`‚ùå Health check failed for ${e}: ${o instanceof Error?o.message:"Unknown error"}`)}});await Promise.allSettled(e),this.updateStats()}decayLoadScores(){for(let e of this.instances.values())e.loadScore*=this.LOAD_DECAY_FACTOR,e.loadScore<.1&&(e.loadScore=0)}updateStats(){let e=Array.from(this.instances.values()),t=e.filter(e=>e.healthy);this.stats.healthyInstances=t.length,this.stats.totalInstances=e.length,t.length>0&&(this.stats.averageResponseTime=t.reduce((e,t)=>e+t.responseTime,0)/t.length),this.stats.lastRebalance=Date.now()}getStats(){let e=Array.from(this.instances.entries()).map(([e,t])=>({url:e,healthy:t.healthy,responseTime:t.responseTime,loadScore:t.loadScore,successRate:t.successCount/Math.max(1,t.successCount+t.errorCount),score:this.calculateInstanceScore(t)}));return{...this.stats,instanceDetails:e.sort((e,t)=>e.score-t.score)}}async forceHealthCheck(){console.log("\uD83D\uDD04 Forcing health check on all instances..."),await this.performHealthChecks();let e=this.getStats();console.log(`üìä Health check complete: ${e.healthyInstances}/${e.totalInstances} healthy instances`)}resetStats(){for(let e of(this.stats={totalRequests:0,totalErrors:0,averageResponseTime:0,healthyInstances:0,totalInstances:0,lastRebalance:Date.now()},this.instances.values()))e.errorCount=0,e.successCount=0,e.loadScore=0,e.consecutiveFailures=0;console.log("\uD83D\uDCCA Load balancer stats reset")}getInstanceHealth(e){return this.instances.get(e)||null}setInstanceHealth(e,t){let s=this.instances.get(e);s&&(s.healthy=t,t&&(s.consecutiveFailures=0),console.log(`üîß Manually set ${e} health to ${t}`),this.updateStats())}}let o=new a((()=>{let e=process.env.OLLAMA_INSTANCES;return e?e.split(",").map(e=>e.trim()):["http://localhost:11434","http://localhost:11435","http://localhost:11436","http://localhost:11437"]})())},9567:(e,t,s)=>{s.d(t,{R:()=>n});var a=s(3506);class o{constructor(){this.cache=new Map,this.DEFAULT_TIMEOUT=parseInt(process.env.OLLAMA_TIMEOUT||"30000"),this.CACHE_TTL=1e3*parseInt(process.env.CACHE_TTL||"3600"),console.log("\uD83E\uDD16 LocalModelService initialized with advanced load balancing"),setInterval(()=>this.cleanExpiredCache(),6e4)}cleanExpiredCache(){let e=Date.now();for(let[t,s]of this.cache.entries())e-s.timestamp>s.ttl&&this.cache.delete(t)}getCached(e){let t=this.cache.get(e);return t?Date.now()-t.timestamp>t.ttl?(this.cache.delete(e),null):t.data:null}setCached(e,t,s=this.CACHE_TTL){this.cache.set(e,{data:t,timestamp:Date.now(),ttl:s})}async getBestInstanceForModel(e){return await a.S.getBestInstanceForModel(e)}createTimeoutController(e=this.DEFAULT_TIMEOUT){let t=new AbortController;return setTimeout(()=>t.abort(),e),t}async generateEmbedding(e,t="nomic-embed-text:latest"){let s=`embedding:${t}:${Buffer.from(e).toString("base64").slice(0,50)}`,o=this.getCached(s);if(o)return console.log("\uD83C\uDFAF Using cached embedding"),o;let n=Date.now(),r="";try{r=await this.getBestInstanceForModel(t);let o=this.createTimeoutController(),l=await fetch(`${r}/api/embeddings`,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({model:t,prompt:e}),signal:o.signal}),i=Date.now()-n;if(!l.ok)throw a.S.recordRequestResult(r,i,!1),Error(`Ollama embeddings failed: ${l.statusText}`);let c=await l.json();return a.S.recordRequestResult(r,i,!0),this.setCached(s,c.embedding),c.embedding}catch(e){if(""===r||/No healthy instances available/.test(String(e)))throw Error(`No Ollama instance with required model '${t}' is available.`);throw console.error("Local embedding generation failed:",e),e}}async generateEmbeddings(e,t="nomic-embed-text:latest"){let s=[];for(let a=0;a<e.length;a+=5){let o=e.slice(a,a+5),n=await Promise.all(o.map(e=>this.generateEmbedding(e,t)));s.push(...n)}return s}async *generateStreamingChatResponse(e,t="llama3.2:1b",s={}){let{temperature:o=.7,max_tokens:n=2048}=s,r=e.map(e=>`${e.role}: ${e.content}`).join("\n")+"\nassistant:",l=Date.now(),i="";try{i=await this.getBestInstanceForModel(t);let e=await fetch(`${i}/api/generate`,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({model:t,prompt:r,options:{temperature:o,num_predict:n},stream:!0})});if(!e.ok){let t=Date.now()-l;throw a.S.recordRequestResult(i,t,!1),Error(`Ollama streaming failed: ${e.statusText}`)}let s=e.body?.getReader();if(!s)throw Error("No response body for streaming");let c=new TextDecoder,h=!0;try{for(;;){let{done:e,value:t}=await s.read();if(e)break;for(let e of c.decode(t).split("\n").filter(e=>e.trim()))try{let t=JSON.parse(e);if(t.response&&(yield t.response),t.done){let e=Date.now()-l;a.S.recordRequestResult(i,e,h);return}}catch(e){}}}catch(e){throw h=!1,e}}catch(e){if(""===i||/No healthy instances available/.test(String(e)))throw Error(`No Ollama instance with required model '${t}' is available.`);if(i){let e=Date.now()-l;a.S.recordRequestResult(i,e,!1)}throw console.error("Local streaming generation failed:",e),e}}async generateChatResponse(e,t="llama3.2:1b",s={}){let{temperature:o=.3,max_tokens:n=1024}=s,r=JSON.stringify(e),l=JSON.stringify({model:t,temperature:o,max_tokens:n}),i=`chat:${Buffer.from(r+l).toString("base64").slice(0,50)}`,c=this.getCached(i);if(c)return console.log("\uD83C\uDFAF Using cached chat response"),c;let h=e.map(e=>`${e.role}: ${e.content}`).join("\n")+"\nassistant:",d=Date.now(),u="";try{u=await this.getBestInstanceForModel(t);let e=this.createTimeoutController(),s=await fetch(`${u}/api/generate`,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({model:t,prompt:h,options:{temperature:o,num_predict:n,top_k:40,top_p:.9,repeat_penalty:1.1,stop:["human:","user:","system:"]},stream:!1}),signal:e.signal}),r=Date.now()-d;if(!s.ok)throw a.S.recordRequestResult(u,r,!1),Error(`Ollama generation failed: ${s.statusText}`);let l=await s.json();return a.S.recordRequestResult(u,r,!0),this.setCached(i,l.response,this.CACHE_TTL),l.response}catch(e){if(""===u||/No healthy instances available/.test(String(e)))throw Error(`No Ollama instance with required model '${t}' is available.`);throw console.error("Local chat generation failed:",e),e}}async isAvailable(){try{return await this.getBestInstanceForModel("llama3.2:1b"),!0}catch{return!1}}async listModels(){try{let e=await this.getBestInstanceForModel("llama3.2:1b"),t=await fetch(`${e}/api/tags`),s=await t.json();return s.models?.map(e=>e.name)||[]}catch(e){return console.error("Failed to list local models:",e),[]}}getCacheStats(){return{size:this.cache.size}}clearCache(){this.cache.clear(),console.log("\uD83E\uDDF9 Local model cache cleared")}async warmupModel(e="llama3.2:1b"){try{return console.log(`üî• Warming up model: ${e}`),await this.generateChatResponse([{role:"user",content:"Hello"}],e,{temperature:.1,max_tokens:10}),console.log(`‚úÖ Model ${e} warmed up successfully`),!0}catch(t){return console.error(`‚ùå Failed to warm up model ${e}:`,t),!1}}}let n=new o}};