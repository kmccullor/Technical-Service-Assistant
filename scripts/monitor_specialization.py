#!/usr/bin/env python3
"""
Specialization Performance Monitor for Technical Service Assistant
Tracks and analyzes performance improvements from model specialization

Monitors:
- Instance utilization distribution
- Response time improvements by question type
- Model selection accuracy and effectiveness
- Resource utilization optimization
"""

import json
import os
import time
from collections import Counter, defaultdict
from dataclasses import asdict, dataclass
from datetime import datetime
from typing import Any, Dict, List

import requests


@dataclass
class RoutingEvent:
    timestamp: float
    question_type: str
    selected_model: str
    selected_instance: str
    instance_url: str
    reasoning: str
    response_time_ms: float = 0.0


class SpecializationMonitor:
    def __init__(self):
        # Allow remote override via RERANKER_URL environment variable
        self.base_url = os.getenv("RERANKER_URL", "http://localhost:8008")
        self.routing_log = []
        self.performance_baseline = {}
        self.monitoring_start = time.time()

    def test_routing_performance(self, test_queries: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Test routing performance with various query types"""
        print("ðŸ” Testing Specialized Routing Performance...")

        results = {
            "total_tests": len(test_queries),
            "routing_distribution": Counter(),
            "response_times": defaultdict(list),
            "model_selection": Counter(),
            "instance_utilization": Counter(),
            "question_type_routing": defaultdict(list),
        }

        for i, test_query in enumerate(test_queries):
            print(f"   Test {i+1}/{len(test_queries)}: {test_query['query'][:50]}...")

            start_time = time.time()

            try:
                # Test routing decision
                routing_response = requests.post(f"{self.base_url}/api/intelligent-route", json=test_query, timeout=10)
                routing_response.raise_for_status()
                routing_data = routing_response.json()

                response_time = (time.time() - start_time) * 1000  # Convert to ms

                # Record routing event
                event = RoutingEvent(
                    timestamp=time.time(),
                    question_type=routing_data["question_type"],
                    selected_model=routing_data["selected_model"],
                    selected_instance=routing_data["selected_instance"],
                    instance_url=routing_data["instance_url"],
                    reasoning=routing_data["reasoning"],
                    response_time_ms=response_time,
                )

                self.routing_log.append(event)

                # Update results
                question_type = routing_data["question_type"]
                selected_instance = routing_data["selected_instance"]
                selected_model = routing_data["selected_model"]

                results["routing_distribution"][selected_instance] += 1
                results["response_times"][question_type].append(response_time)
                results["model_selection"][selected_model] += 1
                results["instance_utilization"][selected_instance] += 1
                results["question_type_routing"][question_type].append(selected_instance)

            except Exception as e:
                print(f"   âŒ Error testing query: {e}")
                continue

        return results

    def analyze_specialization_effectiveness(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze how well specialization is working"""
        print("\nðŸ“Š Analyzing Specialization Effectiveness...")

        analysis = {
            "routing_accuracy": {},
            "instance_specialization_score": {},
            "performance_metrics": {},
            "recommendations": [],
        }

        # Expected routing patterns based on our specialization
        expected_routing = {
            "code": ["ollama-server-2-code-technical"],
            "technical": ["ollama-server-1-chat-qa", "ollama-server-2-code-technical"],
            "math": ["ollama-server-3-reasoning-math"],
            "factual": ["ollama-server-1-chat-qa"],
            "chat": ["ollama-server-1-chat-qa"],
        }

        # Analyze routing accuracy
        for question_type, actual_instances in results["question_type_routing"].items():
            expected_instances = expected_routing.get(question_type, [])
            if expected_instances:
                correct_routes = sum(
                    1 for instance in actual_instances if any(exp in instance for exp in expected_instances)
                )
                accuracy = correct_routes / len(actual_instances) if actual_instances else 0
                analysis["routing_accuracy"][question_type] = {
                    "accuracy": accuracy,
                    "total_queries": len(actual_instances),
                    "correct_routes": correct_routes,
                }

        # Calculate instance specialization scores
        total_queries = sum(results["instance_utilization"].values())
        for instance, query_count in results["instance_utilization"].items():
            utilization_percentage = (query_count / total_queries) * 100
            analysis["instance_specialization_score"][instance] = {
                "query_count": query_count,
                "utilization_percentage": utilization_percentage,
            }

        # Performance metrics
        for question_type, response_times in results["response_times"].items():
            if response_times:
                analysis["performance_metrics"][question_type] = {
                    "avg_response_time": sum(response_times) / len(response_times),
                    "min_response_time": min(response_times),
                    "max_response_time": max(response_times),
                    "query_count": len(response_times),
                }

        # Generate recommendations
        sum(acc["accuracy"] for acc in analysis["routing_accuracy"].values())
        routing_accuracy = analysis["routing_accuracy"]
        accuracy_sum = sum(acc["accuracy"] for acc in routing_accuracy.values())
        total_routing = len(routing_accuracy)
        overall_accuracy = accuracy_sum / total_routing if total_routing else 0
        if overall_accuracy < 0.8:
            analysis["recommendations"].append("Consider refining question classification patterns")

        if len(set(results["instance_utilization"].keys())) < 3:
            analysis["recommendations"].append("Instance distribution could be more balanced")

        # Check if specialization is effective
        code_accuracy = analysis["routing_accuracy"].get("code", {}).get("accuracy", 0)
        if code_accuracy > 0.9:
            analysis["recommendations"].append("Code specialization is working well")

        return analysis

    def generate_performance_report(self, results: Dict[str, Any], analysis: Dict[str, Any]) -> str:
        """Generate comprehensive performance report"""
        report = [
            "# Model Specialization Performance Report",
            f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"Test Duration: {len(self.routing_log)} routing decisions analyzed",
            "",
            "## ðŸŽ¯ Specialization Effectiveness",
            "",
        ]

        # Routing accuracy summary
        if analysis["routing_accuracy"]:
            report.extend(
                [
                    "### Routing Accuracy by Question Type",
                    "| Question Type | Accuracy | Correct/Total | Performance |",
                    "|---------------|----------|---------------|-------------|",
                ]
            )

            for q_type, accuracy_data in analysis["routing_accuracy"].items():
                accuracy_pct = accuracy_data["accuracy"] * 100
                if accuracy_pct >= 90:
                    status = "âœ… Excellent"
                elif accuracy_pct >= 70:
                    status = "âš ï¸ Needs Improvement"
                else:
                    status = "âŒ Poor"
                report.append(
                    "| {type} | {pct:.1f}% | {correct}/{total} | {status} |".format(
                        type=q_type.title(),
                        pct=accuracy_pct,
                        correct=accuracy_data["correct_routes"],
                        total=accuracy_data["total_queries"],
                        status=status,
                    )
                )

        # Instance utilization
        report.extend(
            [
                "",
                "### Instance Utilization Distribution",
                "| Instance | Queries | Utilization | Specialization |",
                "|----------|---------|-------------|----------------|",
            ]
        )

        for instance, utilization_data in analysis["instance_specialization_score"].items():
            util_pct = utilization_data["utilization_percentage"]
            is_specialized = any(term in instance for term in ("code-technical", "reasoning-math", "chat-qa"))
            specialization = "Specialized" if is_specialized else "General"
            report.append(
                "| {instance} | {count} | {util:.1f}% | {label} |".format(
                    instance=instance,
                    count=utilization_data["query_count"],
                    util=util_pct,
                    label=specialization,
                )
            )

        # Performance metrics
        if analysis["performance_metrics"]:
            report.extend(
                [
                    "",
                    "### Response Time Performance",
                    "| Question Type | Avg Response (ms) | Min (ms) | Max (ms) | Queries |",
                    "|---------------|-------------------|----------|----------|---------|",
                ]
            )

            for q_type, perf_data in analysis["performance_metrics"].items():
                report.append(
                    "| {type} | {avg:.1f} | {min:.1f} | {max:.1f} | {count} |".format(
                        type=q_type.title(),
                        avg=perf_data["avg_response_time"],
                        min=perf_data["min_response_time"],
                        max=perf_data["max_response_time"],
                        count=perf_data["query_count"],
                    )
                )

        # Model selection distribution
        report.extend(
            [
                "",
                "### Model Selection Distribution",
                "| Model | Usage Count | Percentage |",
                "|-------|-------------|------------|",
            ]
        )

        total_model_usage = sum(results["model_selection"].values())
        for model, count in results["model_selection"].most_common():
            percentage = (count / total_model_usage) * 100
            report.append(
                "| {model} | {count} | {percentage:.1f}% |".format(
                    model=model,
                    count=count,
                    percentage=percentage,
                )
            )

        # Recommendations
        if analysis["recommendations"]:
            report.extend(["", "## ðŸ“‹ Recommendations", ""])
            for i, rec in enumerate(analysis["recommendations"], 1):
                report.append(f"{i}. {rec}")

        # Summary
        overall_accuracy = total_accuracy / len(analysis["routing_accuracy"])
        total_response_time = sum(sum(times) for times in results["response_times"].values())
        total_response_count = sum(len(times) for times in results["response_times"].values())
        avg_response_time = total_response_time / total_response_count if total_response_count else 0

        status_label = "âœ… Effective" if overall_accuracy > 0.8 else "âš ï¸ Needs Tuning"
        report.extend(
            [
                "",
                "## ðŸ“Š Summary",
                f"- **Overall Routing Accuracy**: {overall_accuracy:.1%}",
                f"- **Average Response Time**: {avg_response_time:.1f}ms",
                f"- **Instances Utilized**: {len(results['instance_utilization'])}/4",
                f"- **Models Used**: {len(results['model_selection'])}",
                f"- **Specialization Status**: {status_label}",
            ]
        )

        return "\\n".join(report)

    def run_comprehensive_test(self) -> str:
        """Run comprehensive specialization test"""
        # Test queries covering different question types
        test_queries = [
            # Code questions (should route to instance 2)
            {
                "query": "How do I write a Python function to parse JSON?",
                "prefer_speed": False,
                "require_context": False,
            },
            {
                "query": "What's the syntax for SQL JOIN statements?",
                "prefer_speed": False,
                "require_context": False,
            },
            {
                "query": "Debug this JavaScript code for me",
                "prefer_speed": False,
                "require_context": False,
            },
            # Technical questions (should route to instance 1 or 2)
            {
                "query": "How do I configure RNI database connections?",
                "prefer_speed": False,
                "require_context": False,
            },
            {
                "query": "Troubleshoot network connectivity issues",
                "prefer_speed": False,
                "require_context": False,
            },
            {
                "query": "Install and setup Active Directory integration",
                "prefer_speed": False,
                "require_context": False,
            },
            # Math questions (should route to instance 3)
            {
                "query": "Calculate the derivative of x^3 + 2x",
                "prefer_speed": False,
                "require_context": False,
            },
            {
                "query": "What is the integral of sin(x)?",
                "prefer_speed": False,
                "require_context": False,
            },
            {
                "query": "Solve this quadratic equation: x^2 + 5x + 6 = 0",
                "prefer_speed": False,
                "require_context": False,
            },
            # Factual questions (should route to instance 1)
            {
                "query": "What is the capital of France?",
                "prefer_speed": False,
                "require_context": False,
            },
            {
                "query": "Explain how photosynthesis works",
                "prefer_speed": False,
                "require_context": False,
            },
            {
                "query": "What are the benefits of renewable energy?",
                "prefer_speed": False,
                "require_context": False,
            },
            # Chat questions (should route to instance 1)
            {
                "query": "Hello, how are you today?",
                "prefer_speed": False,
                "require_context": False,
            },
            {
                "query": "Can you help me plan a vacation?",
                "prefer_speed": False,
                "require_context": False,
            },
            {
                "query": "Tell me a joke",
                "prefer_speed": False,
                "require_context": False,
            },
        ]

        # Run tests
        results = self.test_routing_performance(test_queries)

        # Analyze results
        analysis = self.analyze_specialization_effectiveness(results)

        # Generate report
        report = self.generate_performance_report(results, analysis)

        return report


def main():
    """Main execution function"""
    import argparse

    parser = argparse.ArgumentParser(description="Monitor model specialization performance")
    parser.add_argument("--output", "-o", help="Output file for report")
    parser.add_argument("--json", action="store_true", help="Output raw JSON data")
    args = parser.parse_args()

    print("ðŸš€ Model Specialization Performance Monitor")
    print("=" * 60)

    monitor = SpecializationMonitor()

    if args.json:
        # Run test and output raw JSON
        test_queries = [
            {"query": "How do I write Python code?", "prefer_speed": False, "require_context": False},
            {"query": "Configure RNI server settings", "prefer_speed": False, "require_context": False},
            {"query": "Calculate 2 + 2", "prefer_speed": False, "require_context": False},
        ]
        results = monitor.test_routing_performance(test_queries)
        analysis = monitor.analyze_specialization_effectiveness(results)

        output_data = {
            "results": results,
            "analysis": analysis,
            "routing_log": [asdict(event) for event in monitor.routing_log],
        }

        print(json.dumps(output_data, indent=2, default=str))
    else:
        # Run comprehensive test and generate report
        report = monitor.run_comprehensive_test()

        if args.output:
            with open(args.output, "w") as f:
                f.write(report)
            print(f"\\nðŸ“„ Report saved to: {args.output}")
        else:
            print("\\n" + report)

    return True


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
