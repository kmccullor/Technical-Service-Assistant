#!/usr/bin/env python3
"""
Embedding Fine-Tuner for RNI Domain

Fine-tunes sentence transformers on RNI-specific training data
to improve technical term matching and domain relevance.
"""

import json
import logging
import os
import time
from dataclasses import dataclass
from typing import Any, Dict, List

# Optional imports for fine-tuning
try:
    from sentence_transformers import InputExample, SentenceTransformer, losses
    from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator
    from torch.utils.data import DataLoader

    FINETUNING_AVAILABLE = True
except ImportError:
    print("⚠️  Fine-tuning libraries not available. Install with:")
    print("pip install sentence-transformers torch")
    FINETUNING_AVAILABLE = False

from config import get_settings

logger = logging.getLogger(__name__)
settings = get_settings()


@dataclass
class FineTuningResult:
    """Results from embedding fine-tuning."""

    model_name: str
    training_pairs: int
    epochs_trained: int
    initial_score: float
    final_score: float
    improvement: float
    training_time: float


class EmbeddingFineTuner:
    """Fine-tune embeddings for RNI domain specificity."""

    def __init__(self, base_model: str = "all-MiniLM-L6-v2"):
        """Initialize the fine-tuner."""
        self.base_model = base_model
        self.training_data = self._load_training_data()

        if not FINETUNING_AVAILABLE:
            logger.error("Fine-tuning libraries not available")
            return

        # Load base model
        try:
            self.model = SentenceTransformer(base_model)
            logger.info(f"Loaded base model: {base_model}")
        except Exception as e:
            logger.error(f"Failed to load model {base_model}: {e}")
            self.model = None

    def _load_training_data(self) -> List[Dict]:
        """Load training data generated by Phase 1 setup."""
        try:
            with open("logs/training_data.json", "r") as f:
                return json.load(f)
        except FileNotFoundError:
            logger.error("Training data not found. Run phase1_setup.py first.")
            return []

    def prepare_training_examples(self) -> List[InputExample]:
        """Convert training data to sentence-transformers format."""
        if not FINETUNING_AVAILABLE:
            return []

        examples = []

        for item in self.training_data:
            query = item["query"]
            positive = item["positive_passage"]
            negatives = item["negative_passages"]

            # Create positive example
            examples.append(InputExample(texts=[query, positive], label=1.0))  # High similarity

            # Create negative examples
            for negative in negatives[:2]:  # Limit to 2 negatives per query
                examples.append(InputExample(texts=[query, negative], label=0.1))  # Low similarity

        logger.info(f"Created {len(examples)} training examples")
        return examples

    def create_evaluation_set(self) -> List[InputExample]:
        """Create evaluation set for measuring improvement."""
        if not FINETUNING_AVAILABLE:
            return []

        # Use a subset of training data for evaluation
        eval_examples = []

        # Take every 5th item for evaluation
        eval_data = self.training_data[::5]

        for item in eval_data:
            query = item["query"]
            positive = item["positive_passage"]

            eval_examples.append(InputExample(texts=[query, positive], label=1.0))

            # Add one negative example
            if item["negative_passages"]:
                eval_examples.append(InputExample(texts=[query, item["negative_passages"][0]], label=0.0))

        logger.info(f"Created {len(eval_examples)} evaluation examples")
        return eval_examples

    def fine_tune_model(self, epochs: int = 3, batch_size: int = 8, learning_rate: float = 2e-5) -> FineTuningResult:
        """Fine-tune the embedding model on RNI domain data."""

        if not FINETUNING_AVAILABLE or not self.model:
            logger.error("Fine-tuning not available")
            return FineTuningResult(
                model_name=self.base_model,
                training_pairs=0,
                epochs_trained=0,
                initial_score=0.0,
                final_score=0.0,
                improvement=0.0,
                training_time=0.0,
            )

        print(f"🔧 Fine-tuning {self.base_model} on RNI domain data...")
        start_time = time.time()

        # Prepare training data
        train_examples = self.prepare_training_examples()
        eval_examples = self.create_evaluation_set()

        if not train_examples:
            logger.error("No training examples available")
            return FineTuningResult(
                model_name=self.base_model,
                training_pairs=0,
                epochs_trained=0,
                initial_score=0.0,
                final_score=0.0,
                improvement=0.0,
                training_time=0.0,
            )

        # Create data loader
        train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)

        # Define loss function
        train_loss = losses.CosineSimilarityLoss(self.model)

        # Create evaluator
        evaluator = None
        initial_score = 0.0
        if eval_examples:
            evaluator = EmbeddingSimilarityEvaluator.from_input_examples(eval_examples, name="rni_domain_eval")
            initial_score = evaluator(self.model, output_path=None)

        print(f"📊 Initial evaluation score: {initial_score:.4f}")

        # Fine-tune the model
        warmup_steps = int(len(train_dataloader) * epochs * 0.1)

        self.model.fit(
            train_objectives=[(train_dataloader, train_loss)],
            epochs=epochs,
            warmup_steps=warmup_steps,
            evaluator=evaluator,
            evaluation_steps=len(train_dataloader) // 2,
            output_path=f'logs/rni_finetuned_{self.base_model.replace("/", "_")}',
            save_best_model=True,
        )

        # Final evaluation
        final_score = 0.0
        if evaluator:
            final_score = evaluator(self.model, output_path=None)

        training_time = time.time() - start_time
        improvement = final_score - initial_score

        print(f"✅ Fine-tuning complete!")
        print(f"📈 Final evaluation score: {final_score:.4f}")
        print(f"🚀 Improvement: +{improvement:.4f}")
        print(f"⏱️  Training time: {training_time:.1f}s")

        return FineTuningResult(
            model_name=self.base_model,
            training_pairs=len(self.training_data),
            epochs_trained=epochs,
            initial_score=initial_score,
            final_score=final_score,
            improvement=improvement,
            training_time=training_time,
        )

    def test_domain_improvements(self) -> Dict[str, Any]:
        """Test the fine-tuned model on domain-specific queries."""

        if not FINETUNING_AVAILABLE or not self.model:
            return {"error": "Fine-tuning not available"}

        print("🧪 Testing domain-specific improvements...")

        # Test queries from different categories
        test_queries = {
            "installation": "How to install RNI 4.16",
            "integration": "Active Directory integration with RNI",
            "security": "RNI security configuration setup",
            "troubleshooting": "Fix RNI authentication errors",
            "version": "RNI 4.16.1 release features",
        }

        results = {}

        for category, query in test_queries.items():
            # Find relevant passages from training data
            relevant_passages = []
            for item in self.training_data:
                if item["domain_category"] == category:
                    relevant_passages.append(item["positive_passage"])

            if not relevant_passages:
                continue

            # Generate embeddings
            query_embedding = self.model.encode([query])
            passage_embeddings = self.model.encode(relevant_passages)

            # Calculate similarities
            from sklearn.metrics.pairwise import cosine_similarity

            similarities = cosine_similarity(query_embedding, passage_embeddings)[0]

            results[category] = {
                "query": query,
                "max_similarity": float(max(similarities)),
                "avg_similarity": float(sum(similarities) / len(similarities)),
                "relevant_passages": len(relevant_passages),
            }

        return results

    def save_model_for_ollama(self, model_name: str = "rni-embed-text"):
        """Save the fine-tuned model in a format compatible with Ollama."""

        if not FINETUNING_AVAILABLE or not self.model:
            logger.error("Cannot save model - fine-tuning not available")
            return False

        try:
            # Save as sentence-transformers model
            model_path = f"logs/models/{model_name}"
            os.makedirs(model_path, exist_ok=True)

            self.model.save(model_path)

            print(f"💾 Model saved to: {model_path}")
            print(f"📝 To use with Ollama:")
            print(f"   1. Create Modelfile with sentence-transformers base")
            print(f"   2. Copy model files to Ollama models directory")
            print(f"   3. Use: ollama create {model_name} -f Modelfile")

            return True

        except Exception as e:
            logger.error(f"Failed to save model: {e}")
            return False


class SimpleDomainImprover:
    """Simple improvements without complex dependencies."""

    def __init__(self):
        """Initialize simple domain improver."""
        self.training_data = self._load_training_data()

    def _load_training_data(self) -> List[Dict]:
        """Load training data."""
        try:
            with open("logs/training_data.json", "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []

    def create_domain_term_glossary(self) -> Dict[str, List[str]]:
        """Create a glossary of domain-specific terms for query expansion."""

        print("📚 Creating domain term glossary...")

        glossary = {
            "rni_versions": ["4.16", "4.16.1", "4.15", "4.14"],
            "installation_terms": [
                "install",
                "installation",
                "setup",
                "configure",
                "configuration",
                "deploy",
                "deployment",
                "initialize",
                "provision",
            ],
            "integration_terms": [
                "Active Directory",
                "LDAP",
                "API",
                "connector",
                "interface",
                "integration",
                "sync",
                "authentication",
                "SSO",
            ],
            "security_terms": [
                "security",
                "authentication",
                "authorization",
                "certificate",
                "encryption",
                "SSL",
                "TLS",
                "firewall",
                "access control",
            ],
            "troubleshooting_terms": [
                "error",
                "issue",
                "problem",
                "troubleshoot",
                "debug",
                "fix",
                "resolve",
                "diagnose",
                "repair",
            ],
        }

        # Save glossary
        with open("logs/domain_glossary.json", "w") as f:
            json.dump(glossary, f, indent=2)

        print(f"✅ Domain glossary created with {sum(len(terms) for terms in glossary.values())} terms")
        return glossary

    def enhance_query_expansion(self, query: str, glossary: Dict[str, List[str]]) -> str:
        """Enhance queries using domain glossary."""

        enhanced_query = query.lower()

        # Add related terms
        for category, terms in glossary.items():
            for term in terms:
                if term.lower() in enhanced_query:
                    # Add related terms from the same category
                    related_terms = [t for t in terms if t.lower() != term.lower()]
                    if related_terms:
                        enhanced_query += f" {related_terms[0]}"
                    break

        return enhanced_query


def main():
    """Run embedding fine-tuning for 90% accuracy target."""
    print("🎯 Embedding Fine-Tuning for 90% Accuracy")
    print("=" * 60)

    # Check if fine-tuning is available
    if FINETUNING_AVAILABLE:
        print("✅ Fine-tuning libraries available")

        # Initialize fine-tuner
        fine_tuner = EmbeddingFineTuner("all-MiniLM-L6-v2")

        if not fine_tuner.training_data:
            print("❌ No training data found. Run: DB_HOST=localhost python phase1_setup.py")
            return

        print(f"📊 Training data loaded: {len(fine_tuner.training_data)} pairs")

        # Fine-tune the model
        result = fine_tuner.fine_tune_model(epochs=3, batch_size=8)

        if result.improvement > 0:
            print(f"\n🎉 Fine-tuning successful!")
            print(f"📈 Improvement: +{result.improvement:.4f}")

            # Test domain improvements
            domain_results = fine_tuner.test_domain_improvements()

            print(f"\n🧪 Domain Testing Results:")
            for category, data in domain_results.items():
                print(f"  {category}: max_sim={data['max_similarity']:.3f}, avg_sim={data['avg_similarity']:.3f}")

            # Save model
            fine_tuner.save_model_for_ollama("rni-embed-text")

            # Calculate expected accuracy improvement
            base_accuracy = 0.82  # Current baseline
            expected_improvement = result.improvement * 0.1  # Conservative estimate
            projected_accuracy = base_accuracy + expected_improvement

            print(f"\n📊 Projected Impact:")
            print(f"  Current accuracy: {base_accuracy:.1%}")
            print(f"  Expected improvement: +{expected_improvement:.1%}")
            print(f"  Projected accuracy: {projected_accuracy:.1%}")

        else:
            print("⚠️  Fine-tuning did not show improvement. Consider:")
            print("  - More training data")
            print("  - Different base model")
            print("  - Adjusted hyperparameters")

    else:
        print("⚠️  Fine-tuning libraries not available")
        print("📚 Creating domain improvements without fine-tuning...")

        # Use simple improvements
        improver = SimpleDomainImprover()

        if not improver.training_data:
            print("❌ No training data found. Run: DB_HOST=localhost python phase1_setup.py")
            return

        # Create domain glossary
        glossary = improver.create_domain_term_glossary()

        # Test query expansion
        test_queries = ["RNI installation", "Active Directory setup", "security configuration", "troubleshoot errors"]

        print(f"\n🔍 Query Enhancement Examples:")
        for query in test_queries:
            enhanced = improver.enhance_query_expansion(query, glossary)
            print(f"  '{query}' → '{enhanced}'")

        print(f"\n📈 Simple improvements ready for integration!")

    print(f"\n🎯 Next Steps:")
    print(f"  1. Integrate improved embeddings with enhanced_retrieval.py")
    print(f"  2. Implement multi-stage reranking")
    print(f"  3. Test accuracy improvements")
    print(f"  4. Deploy ensemble approach")


if __name__ == "__main__":
    main()
